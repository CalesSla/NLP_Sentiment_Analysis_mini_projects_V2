{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7fb6e7",
   "metadata": {},
   "source": [
    "# IMBD reviews model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906e89d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "import itertools\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D, BatchNormalization, Dense, RNN, GRU, LSTM, TimeDistributed, Bidirectional, Activation, Embedding, Input, Conv1D, Dropout\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c45645",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imports\n",
    "with open('reviews.txt', 'r') as f:\n",
    "  reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "  labels = f.read()\n",
    "  \n",
    "# Remove punctuation\n",
    "reviews = \"\".join([char for char in reviews if char not in string.punctuation])\n",
    "\n",
    "reviews = reviews.split('\\n')\n",
    "labels = labels.split('\\n')\n",
    "\n",
    "# Tokenization, Lemmatization, Stemming, Stopwords. Label numerical encoding\n",
    "reviews_tokenized = []\n",
    "reviews_tokenized_joined = []\n",
    "for review in reviews:\n",
    "  splitted_review = nltk.word_tokenize(review)\n",
    "  splitted_review = [word for word in splitted_review if word not in stop_words]\n",
    "  splitted_review = [WordNetLemmatizer().lemmatize(w) for w in splitted_review]\n",
    "  splitted_review = [PorterStemmer().stem(w).strip() for w in splitted_review]\n",
    "  reviews_tokenized.append(splitted_review)\n",
    "  joined_review = ' '.join(splitted_review)\n",
    "  reviews_tokenized_joined.append(joined_review)\n",
    "  \n",
    "# Remove empty reviews and the corresponding labels\n",
    "empty_idx = []\n",
    "for i, review in enumerate(reviews_tokenized):\n",
    "  if len(review) == 0:\n",
    "    empty_idx.append(i)\n",
    "    \n",
    "for i in empty_idx:\n",
    "  reviews_tokenized.pop(i)\n",
    "  reviews_tokenized_joined.pop(i)\n",
    "  reviews.pop(i)\n",
    "  labels.pop(i)\n",
    "  \n",
    "reviews_unrolled = list(itertools.chain(*reviews_tokenized))\n",
    "labels = [1 if label == \"positive\" else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2e12d",
   "metadata": {},
   "source": [
    "### Part 1. Multinomial Naive Bayes with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebe550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB CountVectorizer Training time: 2.36 seconds\n",
      "MultinomialNB CountVectorizer Accuracy score:  0.8582\n",
      "MultinomialNB CountVectorizer Precision score:  0.8621118012422361\n",
      "MultinomialNB CountVectorizer Recall score:  0.8470301057770545\n",
      "MultinomialNB CountVectorizer F1 score:  0.8545044120664889\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "MultinomialNB_CountVectorizer_start_time = time.time()\n",
    "\n",
    "# Count vectorizer feature transformation\n",
    "count_vector = CountVectorizer(stop_words = 'english', binary = False)\n",
    "\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "test_data = count_vector.transform(X_test)\n",
    "\n",
    "# Multinomial Naive Bayes model predictions\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "MultinomialNB_CountVectorizer_predictions = naive_bayes.predict(test_data)\n",
    "\n",
    "MultinomialNB_CountVectorizer_end_time = time.time()\n",
    "MultinomialNB_CountVectorizer_execution_time = MultinomialNB_CountVectorizer_end_time - MultinomialNB_CountVectorizer_start_time\n",
    "\n",
    "# Model evaluation\n",
    "print('MultinomialNB CountVectorizer Training time: {} seconds'. format(round(MultinomialNB_CountVectorizer_execution_time, 2)))\n",
    "print('MultinomialNB CountVectorizer Accuracy score: ', format(accuracy_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "print('MultinomialNB CountVectorizer Precision score: ', format(precision_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "print('MultinomialNB CountVectorizer Recall score: ', format(recall_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "print('MultinomialNB CountVectorizer F1 score: ', format(f1_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "MultinomialNB_CountVectorizer_results = {'Name': \"MultinomialNB CountVectorizer\", \n",
    "                                         \"Training Time\": round(MultinomialNB_CountVectorizer_execution_time, 2),\n",
    "                                         \"Accuracy score\": accuracy_score(y_test, MultinomialNB_CountVectorizer_predictions),\n",
    "                                         \"Precision score\": precision_score(y_test, MultinomialNB_CountVectorizer_predictions),\n",
    "                                         \"Recall score\": recall_score(y_test, MultinomialNB_CountVectorizer_predictions),\n",
    "                                         \"F1 score\": f1_score(y_test, MultinomialNB_CountVectorizer_predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ff3ed",
   "metadata": {},
   "source": [
    "### Part 2. Multinomial Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b11e4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB tfidf Training time: 2.34 seconds\n",
      "MultinomialNB tfidf Accuracy score:  0.864\n",
      "MultinomialNB tfidf Precision score:  0.8587570621468926\n",
      "MultinomialNB tfidf Recall score:  0.8657445077298617\n",
      "MultinomialNB tfidf F1 score:  0.8622366288492707\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "MultinomialNB_tfidf_start_time = time.time()\n",
    "\n",
    "# TF-IDF feature transformation\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', binary = False)\n",
    "\n",
    "training_data = tfidf.fit_transform(X_train)\n",
    "test_data = tfidf.transform(X_test)\n",
    "\n",
    "# Multinomial Naive Bayes model predictions\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "MultinomialNB_tfidf_predictions = naive_bayes.predict(test_data)\n",
    "\n",
    "MultinomialNB_tfidf_end_time = time.time()\n",
    "MultinomialNB_tfidf_execution_time = MultinomialNB_tfidf_end_time - MultinomialNB_tfidf_start_time\n",
    "\n",
    "# Model evaluation\n",
    "print('MultinomialNB tfidf Training time: {} seconds'. format(round(MultinomialNB_tfidf_execution_time, 2)))\n",
    "print('MultinomialNB tfidf Accuracy score: ', format(accuracy_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "print('MultinomialNB tfidf Precision score: ', format(precision_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "print('MultinomialNB tfidf Recall score: ', format(recall_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "print('MultinomialNB tfidf F1 score: ', format(f1_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "MultinomialNB_tfidf_results = {'Name': \"MultinomialNB tfidf\", \n",
    "                                         \"Training Time\": round(MultinomialNB_tfidf_execution_time, 2),\n",
    "                                         \"Accuracy score\": accuracy_score(y_test, MultinomialNB_tfidf_predictions),\n",
    "                                         \"Precision score\": precision_score(y_test, MultinomialNB_tfidf_predictions),\n",
    "                                         \"Recall score\": recall_score(y_test, MultinomialNB_tfidf_predictions),\n",
    "                                         \"F1 score\": f1_score(y_test, MultinomialNB_tfidf_predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b33ad1",
   "metadata": {},
   "source": [
    "### Part 3. Vaders sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590d756",
   "metadata": {},
   "source": [
    "#### Extra preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a669269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high is a cartoon comedy  it ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>brilliant over  acting by lesley ann warren  b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text  Label\n",
       "0   1  bromwell high is a cartoon comedy  it ran at t...      1\n",
       "1   2  story of a man who has unnatural feelings for ...      0\n",
       "2   3  homelessness  or houselessness as george carli...      1\n",
       "3   4  airport    starts as a brand new luxury    pla...      0\n",
       "4   5  brilliant over  acting by lesley ann warren  b...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe for convenience\n",
    "\n",
    "df = pd.DataFrame({'Text': reviews, 'Label': labels})\n",
    "df = df.reset_index()\n",
    "df['Id'] = df['index'] + 1\n",
    "df.drop('index', axis = 1, inplace = True)\n",
    "df = df[['Id', 'Text', 'Label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ff1cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180fa7cc4e72404c91254f7181aa11f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Predicted_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>bromwell high is a cartoon comedy  it ran at t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.9180</td>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>brilliant over  acting by lesley ann warren  b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    neg    neu    pos  compound  \\\n",
       "0   1  0.044  0.916  0.041   -0.1027   \n",
       "1   2  0.108  0.746  0.146    0.7003   \n",
       "2   3  0.120  0.733  0.147    0.9311   \n",
       "3   4  0.161  0.692  0.147   -0.9180   \n",
       "4   5  0.077  0.738  0.185    0.9657   \n",
       "\n",
       "                                                Text  Label  Predicted_Label  \n",
       "0  bromwell high is a cartoon comedy  it ran at t...      1                0  \n",
       "1  story of a man who has unnatural feelings for ...      0                1  \n",
       "2  homelessness  or houselessness as george carli...      1                1  \n",
       "3  airport    starts as a brand new luxury    pla...      0                0  \n",
       "4  brilliant over  acting by lesley ann warren  b...      1                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_start_time = time.time()\n",
    "\n",
    "# Define Vader Sentiment analyzer model\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Get the results\n",
    "res = {}\n",
    "for i, row in tqdm(df.iterrows(), total = len(df)):\n",
    "  text = row['Text']\n",
    "  myid = row['Id']\n",
    "  res[myid] = sia.polarity_scores(text)\n",
    "  \n",
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns = {'index': 'Id'})\n",
    "vaders = vaders.merge(df, how = 'left')\n",
    "vaders['Predicted_Label'] = np.where(vaders['compound'] >= 0, 1, 0)\n",
    "\n",
    "vader_end_time = time.time()\n",
    "vader_execution_time = vader_end_time - vader_start_time\n",
    "\n",
    "vaders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68236eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader Training time: 37.44 seconds\n",
      "Vader Accuracy score:  0.69088\n",
      "Vader Precision score:  0.6435101648021172\n",
      "Vader Recall score:  0.85592\n",
      "Vader F1 score:  0.7346700542470646\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "print('Vader Training time: {} seconds'. format(round(vader_execution_time, 2)))\n",
    "print('Vader Accuracy score: ', format(accuracy_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "print('Vader Precision score: ', format(precision_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "print('Vader Recall score: ', format(recall_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "print('Vader F1 score: ', format(f1_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "vader_results = {'Name': \"Vaders Analyser\", \n",
    "                                         \"Training Time\": round(vader_execution_time, 2),\n",
    "                                         \"Accuracy score\": accuracy_score(vaders['Label'], vaders['Predicted_Label']),\n",
    "                                         \"Precision score\": precision_score(vaders['Label'], vaders['Predicted_Label']),\n",
    "                                         \"Recall score\": recall_score(vaders['Label'], vaders['Predicted_Label']),\n",
    "                                         \"F1 score\": f1_score(vaders['Label'], vaders['Predicted_Label'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902d703",
   "metadata": {},
   "source": [
    "### Part 4. RNN manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary, word2index reference and convert the reviews into numerical form\n",
    "vocab_size = 10000\n",
    "\n",
    "word_counter = Counter(reviews_unrolled)\n",
    "word_counter = dict(word_counter.most_common(vocab_size))\n",
    "word2index = {k:i for i,k in enumerate(word_counter.keys(), start = 3)}\n",
    "\n",
    "# Convert reveies to integers\n",
    "reviews_int = []\n",
    "for review in reviews_tokenized:\n",
    "  cur_review = [1]\n",
    "  for word in review:\n",
    "    if word in word2index.keys():\n",
    "      cur_review.append(word2index[word])\n",
    "    else:\n",
    "      cur_review.append(2)\n",
    "  reviews_int.append(cur_review)\n",
    "  \n",
    "# Pad sequences\n",
    "padded_reviews = pad_sequences(reviews_int, maxlen = 500, padding = 'pre', truncating = 'pre')\n",
    "\n",
    "# Train test split on padded sequences\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "X_train = np.array(X_train).reshape(20000, 500)\n",
    "X_test = np.array(X_test).reshape(5000, 500)\n",
    "\n",
    "y_train = np.array(y_train).reshape(20000, 1)\n",
    "y_test = np.array(y_test).reshape(5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e53bab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 200)          333000    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 500, 200)         800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 200)          0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 500, 128)          126720    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 500, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500, 128)          0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,177\n",
      "Trainable params: 1,906,265\n",
      "Non-trainable params: 912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(filepath='best_RNN_manual_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=3, \n",
    "                           verbose=1, \n",
    "                           restore_best_weights=True)\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1:]))\n",
    "mask = tf.keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "x = Embedding(input_dim = vocab_size, output_dim = 128, input_length = 200)(inputs)\n",
    "x = Conv1D(filters = 200, kernel_size = 13, strides = 1, padding = 'same', activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = GRU(128, return_sequences = True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = GRU(128, return_sequences = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac97258f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 26s 128ms/step - loss: 0.6668 - accuracy: 0.6176 - val_loss: 0.4608 - val_accuracy: 0.8078\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 20s 128ms/step - loss: 0.2946 - accuracy: 0.8820 - val_loss: 0.4178 - val_accuracy: 0.8190\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 20s 130ms/step - loss: 0.1842 - accuracy: 0.9313 - val_loss: 0.3449 - val_accuracy: 0.8548\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.1255 - accuracy: 0.9548 - val_loss: 0.4050 - val_accuracy: 0.8494\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 18s 116ms/step - loss: 0.0787 - accuracy: 0.9731 - val_loss: 0.7380 - val_accuracy: 0.8150\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9804Restoring model weights from the end of the best epoch: 3.\n",
      "157/157 [==============================] - 19s 118ms/step - loss: 0.0558 - accuracy: 0.9804 - val_loss: 0.5631 - val_accuracy: 0.8640\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "Manual_RNN_start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 10, batch_size = 128, validation_data = (X_test, y_test), callbacks = [checkpoint, early_stop])\n",
    "\n",
    "Manual_RNN_end_time = time.time()\n",
    "Manual_RNN_execution_time = Manual_RNN_end_time - Manual_RNN_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e32bedec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 18ms/step\n",
      "Manual RNN Training time: 122.5 seconds\n",
      "Manual RNN Accuracy score:  0.8548\n",
      "Manual RNN Precision score:  0.923679060665362\n",
      "Manual RNN Recall score:  0.7681041497152156\n",
      "Manual RNN F1 score:  0.838738338516215\n"
     ]
    }
   ],
   "source": [
    "# Model predictions\n",
    "Manual_RNN_predictions = model.predict(X_test)\n",
    "\n",
    "Manual_RNN_predictions_results = []\n",
    "for i in Manual_RNN_predictions:\n",
    "  if i[0] >= 0.5:\n",
    "    Manual_RNN_predictions_results.append(1)\n",
    "  else:\n",
    "    Manual_RNN_predictions_results.append(0)\n",
    "\n",
    "# Model evaluation\n",
    "print('Manual RNN Training time: {} seconds'. format(round(Manual_RNN_execution_time, 2)))\n",
    "print('Manual RNN Accuracy score: ', format(accuracy_score(y_test, Manual_RNN_predictions_results)))\n",
    "print('Manual RNN Precision score: ', format(precision_score(y_test, Manual_RNN_predictions_results)))\n",
    "print('Manual RNN Recall score: ', format(recall_score(y_test, Manual_RNN_predictions_results)))\n",
    "print('Manual RNN F1 score: ', format(f1_score(y_test, Manual_RNN_predictions_results)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "Manual_RNN_results = {'Name': \"Manual RNN\", \n",
    "                 \"Training Time\": round(Manual_RNN_execution_time, 2),\n",
    "                 \"Accuracy score\": accuracy_score(y_test, Manual_RNN_predictions_results),\n",
    "                 \"Precision score\": precision_score(y_test, Manual_RNN_predictions_results),\n",
    "                 \"Recall score\": recall_score(y_test, Manual_RNN_predictions_results),\n",
    "                 \"F1 score\": f1_score(y_test, Manual_RNN_predictions_results)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b84ef",
   "metadata": {},
   "source": [
    "### Part 5. Universal Sentence encoder transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e84cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train).reshape(20000, 1)\n",
    "y_test = np.array(y_test).reshape(5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de4b6886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258,373,761\n",
      "Trainable params: 1,575,937\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Download the Universal Sentence encoder layer for transfer learning\n",
    "sentence_encoder_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                                         input_shape = [],\n",
    "                                         dtype=tf.string,\n",
    "                                         trainable = False)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(filepath='best_sentence_encoder_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=3, \n",
    "                           verbose=1, \n",
    "                           restore_best_weights=True)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "  sentence_encoder_layer,\n",
    "  Dense(1024, activation = 'relu'),\n",
    "  Dense(1024, activation = 'relu'),\n",
    "  Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff06526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 23s 138ms/step - loss: 0.3632 - accuracy: 0.8382 - val_loss: 0.3222 - val_accuracy: 0.8600\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 20s 131ms/step - loss: 0.3149 - accuracy: 0.8665 - val_loss: 0.3133 - val_accuracy: 0.8680\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 19s 118ms/step - loss: 0.2854 - accuracy: 0.8806 - val_loss: 0.3538 - val_accuracy: 0.8484\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 21s 131ms/step - loss: 0.2535 - accuracy: 0.8965 - val_loss: 0.3107 - val_accuracy: 0.8718\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 18s 118ms/step - loss: 0.2033 - accuracy: 0.9226 - val_loss: 0.3287 - val_accuracy: 0.8672\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.1641 - accuracy: 0.9392 - val_loss: 0.3789 - val_accuracy: 0.8462\n",
      "Epoch 7/10\n",
      "156/157 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9570Restoring model weights from the end of the best epoch: 4.\n",
      "157/157 [==============================] - 19s 121ms/step - loss: 0.1167 - accuracy: 0.9571 - val_loss: 0.3947 - val_accuracy: 0.8636\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "Sentence_encoder_start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 10, batch_size = 128, validation_data = (X_test, y_test), callbacks = [checkpoint, early_stop])\n",
    "\n",
    "Sentence_encoder_end_time = time.time()\n",
    "Sentence_encoder_execution_time = Sentence_encoder_end_time - Sentence_encoder_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "875e96b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 22ms/step\n",
      "Sentence encoder Training time: 139.24 seconds\n",
      "Sentence encoder Accuracy score:  0.8718\n",
      "Sentence encoder Precision score:  0.8799665411961523\n",
      "Sentence encoder Recall score:  0.8559804719283971\n",
      "Sentence encoder F1 score:  0.8678077954217365\n"
     ]
    }
   ],
   "source": [
    "# Model predictions\n",
    "Sentence_encoder_predictions = model.predict(X_test)\n",
    "\n",
    "Sentence_encoder_predictions_results = []\n",
    "for i in Sentence_encoder_predictions:\n",
    "  if i[0] >= 0.5:\n",
    "    Sentence_encoder_predictions_results.append(1)\n",
    "  else:\n",
    "    Sentence_encoder_predictions_results.append(0)\n",
    "    \n",
    "# Model evaluation\n",
    "print('Sentence encoder Training time: {} seconds'. format(round(Sentence_encoder_execution_time, 2)))\n",
    "print('Sentence encoder Accuracy score: ', format(accuracy_score(y_test, Sentence_encoder_predictions_results)))\n",
    "print('Sentence encoder Precision score: ', format(precision_score(y_test, Sentence_encoder_predictions_results)))\n",
    "print('Sentence encoder Recall score: ', format(recall_score(y_test, Sentence_encoder_predictions_results)))\n",
    "print('Sentence encoder F1 score: ', format(f1_score(y_test, Sentence_encoder_predictions_results)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "Manual_RNN_results = {'Name': \"Sentence encoder\", \n",
    "                 \"Training Time\": round(Sentence_encoder_execution_time, 2),\n",
    "                 \"Accuracy score\": accuracy_score(y_test, Sentence_encoder_predictions_results),\n",
    "                 \"Precision score\": precision_score(y_test, Sentence_encoder_predictions_results),\n",
    "                 \"Recall score\": recall_score(y_test, Sentence_encoder_predictions_results),\n",
    "                 \"F1 score\": f1_score(y_test, Sentence_encoder_predictions_results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0630d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7af96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
