{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7fb6e7",
   "metadata": {},
   "source": [
    "# IMBD reviews model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906e89d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "import itertools\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D, BatchNormalization, Dense, RNN, GRU, LSTM, TimeDistributed, Bidirectional, Activation, Embedding, Input, Conv1D, Dropout\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c45645",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c6b0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imports\n",
    "with open('reviews.txt', 'r') as f:\n",
    "  reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "  labels = f.read()\n",
    "  \n",
    "# Remove punctuation\n",
    "reviews = \"\".join([char for char in reviews if char not in string.punctuation])\n",
    "\n",
    "reviews = reviews.split('\\n')\n",
    "labels = labels.split('\\n')\n",
    "\n",
    "# Tokenization, Lemmatization, Stemming, Stopwords. Label numerical encoding\n",
    "reviews_tokenized = []\n",
    "reviews_tokenized_joined = []\n",
    "for review in reviews:\n",
    "  splitted_review = nltk.word_tokenize(review)\n",
    "  splitted_review = [word for word in splitted_review if word not in stop_words]\n",
    "  splitted_review = [WordNetLemmatizer().lemmatize(w) for w in splitted_review]\n",
    "  splitted_review = [PorterStemmer().stem(w).strip() for w in splitted_review]\n",
    "  reviews_tokenized.append(splitted_review)\n",
    "  joined_review = ' '.join(splitted_review)\n",
    "  reviews_tokenized_joined.append(joined_review)\n",
    "  \n",
    "# Remove empty reviews and the corresponding labels\n",
    "empty_idx = []\n",
    "for i, review in enumerate(reviews_tokenized):\n",
    "  if len(review) == 0:\n",
    "    empty_idx.append(i)\n",
    "    \n",
    "for i in empty_idx:\n",
    "  reviews_tokenized.pop(i)\n",
    "  reviews_tokenized_joined.pop(i)\n",
    "  reviews.pop(i)\n",
    "  labels.pop(i)\n",
    "  \n",
    "reviews_unrolled = list(itertools.chain(*reviews_tokenized))\n",
    "labels = [1 if label == \"positive\" else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c2e12d",
   "metadata": {},
   "source": [
    "### Part 1. Multinomial Naive Bayes with Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebe550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB CountVectorizer Training time: 2.13 seconds\n",
      "MultinomialNB CountVectorizer Accuracy score:  0.8582\n",
      "MultinomialNB CountVectorizer Precision score:  0.8621118012422361\n",
      "MultinomialNB CountVectorizer Recall score:  0.8470301057770545\n",
      "MultinomialNB CountVectorizer F1 score:  0.8545044120664889\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "MultinomialNB_CountVectorizer_start_time = time.time()\n",
    "\n",
    "# Count vectorizer feature transformation\n",
    "count_vector = CountVectorizer(stop_words = 'english', binary = False)\n",
    "\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "test_data = count_vector.transform(X_test)\n",
    "\n",
    "# Multinomial Naive Bayes model predictions\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "MultinomialNB_CountVectorizer_predictions = naive_bayes.predict(test_data)\n",
    "\n",
    "MultinomialNB_CountVectorizer_end_time = time.time()\n",
    "MultinomialNB_CountVectorizer_execution_time = MultinomialNB_CountVectorizer_end_time - MultinomialNB_CountVectorizer_start_time\n",
    "\n",
    "# Model evaluation\n",
    "print('MultinomialNB CountVectorizer Training time: {} seconds'. format(round(MultinomialNB_CountVectorizer_execution_time, 2)))\n",
    "print('MultinomialNB CountVectorizer Accuracy score: ', format(accuracy_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "print('MultinomialNB CountVectorizer Precision score: ', format(precision_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "print('MultinomialNB CountVectorizer Recall score: ', format(recall_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "print('MultinomialNB CountVectorizer F1 score: ', format(f1_score(y_test, MultinomialNB_CountVectorizer_predictions)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "MultinomialNB_CountVectorizer_results = {'Name': \"MultinomialNB CountVectorizer\", \n",
    "                                         \"Training Time\": round(MultinomialNB_CountVectorizer_execution_time, 2),\n",
    "                                         \"Accuracy score\": accuracy_score(y_test, MultinomialNB_CountVectorizer_predictions),\n",
    "                                         \"Precision score\": precision_score(y_test, MultinomialNB_CountVectorizer_predictions),\n",
    "                                         \"Recall score\": recall_score(y_test, MultinomialNB_CountVectorizer_predictions),\n",
    "                                         \"F1 score\": f1_score(y_test, MultinomialNB_CountVectorizer_predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ff3ed",
   "metadata": {},
   "source": [
    "### Part 2. Multinomial Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b11e4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB tfidf Training time: 2.18 seconds\n",
      "MultinomialNB tfidf Accuracy score:  0.864\n",
      "MultinomialNB tfidf Precision score:  0.8587570621468926\n",
      "MultinomialNB tfidf Recall score:  0.8657445077298617\n",
      "MultinomialNB tfidf F1 score:  0.8622366288492707\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "MultinomialNB_tfidf_start_time = time.time()\n",
    "\n",
    "# TF-IDF feature transformation\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', binary = False)\n",
    "\n",
    "training_data = tfidf.fit_transform(X_train)\n",
    "test_data = tfidf.transform(X_test)\n",
    "\n",
    "# Multinomial Naive Bayes model predictions\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data, y_train)\n",
    "\n",
    "MultinomialNB_tfidf_predictions = naive_bayes.predict(test_data)\n",
    "\n",
    "MultinomialNB_tfidf_end_time = time.time()\n",
    "MultinomialNB_tfidf_execution_time = MultinomialNB_tfidf_end_time - MultinomialNB_tfidf_start_time\n",
    "\n",
    "# Model evaluation\n",
    "print('MultinomialNB tfidf Training time: {} seconds'. format(round(MultinomialNB_tfidf_execution_time, 2)))\n",
    "print('MultinomialNB tfidf Accuracy score: ', format(accuracy_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "print('MultinomialNB tfidf Precision score: ', format(precision_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "print('MultinomialNB tfidf Recall score: ', format(recall_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "print('MultinomialNB tfidf F1 score: ', format(f1_score(y_test, MultinomialNB_tfidf_predictions)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "MultinomialNB_tfidf_results = {'Name': \"MultinomialNB tfidf\", \n",
    "                                         \"Training Time\": round(MultinomialNB_tfidf_execution_time, 2),\n",
    "                                         \"Accuracy score\": accuracy_score(y_test, MultinomialNB_tfidf_predictions),\n",
    "                                         \"Precision score\": precision_score(y_test, MultinomialNB_tfidf_predictions),\n",
    "                                         \"Recall score\": recall_score(y_test, MultinomialNB_tfidf_predictions),\n",
    "                                         \"F1 score\": f1_score(y_test, MultinomialNB_tfidf_predictions)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b33ad1",
   "metadata": {},
   "source": [
    "### Part 3. Vaders sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590d756",
   "metadata": {},
   "source": [
    "#### Extra preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a669269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>bromwell high is a cartoon comedy  it ran at t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>brilliant over  acting by lesley ann warren  b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               Text  Label\n",
       "0   1  bromwell high is a cartoon comedy  it ran at t...      1\n",
       "1   2  story of a man who has unnatural feelings for ...      0\n",
       "2   3  homelessness  or houselessness as george carli...      1\n",
       "3   4  airport    starts as a brand new luxury    pla...      0\n",
       "4   5  brilliant over  acting by lesley ann warren  b...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe for convenience\n",
    "\n",
    "df = pd.DataFrame({'Text': reviews, 'Label': labels})\n",
    "df = df.reset_index()\n",
    "df['Id'] = df['index'] + 1\n",
    "df.drop('index', axis = 1, inplace = True)\n",
    "df = df[['Id', 'Text', 'Label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ff1cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9714e509436e44ee8db48093caee123e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Predicted_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>bromwell high is a cartoon comedy  it ran at t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>story of a man who has unnatural feelings for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>homelessness  or houselessness as george carli...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-0.9180</td>\n",
       "      <td>airport    starts as a brand new luxury    pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.9657</td>\n",
       "      <td>brilliant over  acting by lesley ann warren  b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    neg    neu    pos  compound  \\\n",
       "0   1  0.044  0.916  0.041   -0.1027   \n",
       "1   2  0.108  0.746  0.146    0.7003   \n",
       "2   3  0.120  0.733  0.147    0.9311   \n",
       "3   4  0.161  0.692  0.147   -0.9180   \n",
       "4   5  0.077  0.738  0.185    0.9657   \n",
       "\n",
       "                                                Text  Label  Predicted_Label  \n",
       "0  bromwell high is a cartoon comedy  it ran at t...      1                0  \n",
       "1  story of a man who has unnatural feelings for ...      0                1  \n",
       "2  homelessness  or houselessness as george carli...      1                1  \n",
       "3  airport    starts as a brand new luxury    pla...      0                0  \n",
       "4  brilliant over  acting by lesley ann warren  b...      1                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_start_time = time.time()\n",
    "\n",
    "# Define Vader Sentiment analyzer model\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Get the results\n",
    "res = {}\n",
    "for i, row in tqdm(df.iterrows(), total = len(df)):\n",
    "  text = row['Text']\n",
    "  myid = row['Id']\n",
    "  res[myid] = sia.polarity_scores(text)\n",
    "  \n",
    "vaders = pd.DataFrame(res).T\n",
    "vaders = vaders.reset_index().rename(columns = {'index': 'Id'})\n",
    "vaders = vaders.merge(df, how = 'left')\n",
    "vaders['Predicted_Label'] = np.where(vaders['compound'] >= 0, 1, 0)\n",
    "\n",
    "vader_end_time = time.time()\n",
    "vader_execution_time = vader_end_time - vader_start_time\n",
    "\n",
    "vaders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68236eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader Training time: 34.85 seconds\n",
      "Vader Accuracy score:  0.69088\n",
      "Vader Precision score:  0.6435101648021172\n",
      "Vader Recall score:  0.85592\n",
      "Vader F1 score:  0.7346700542470646\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "print('Vader Training time: {} seconds'. format(round(vader_execution_time, 2)))\n",
    "print('Vader Accuracy score: ', format(accuracy_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "print('Vader Precision score: ', format(precision_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "print('Vader Recall score: ', format(recall_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "print('Vader F1 score: ', format(f1_score(vaders['Label'], vaders['Predicted_Label'])))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "vader_results = {'Name': \"Vaders Analyser\", \n",
    "                                         \"Training Time\": round(vader_execution_time, 2),\n",
    "                                         \"Accuracy score\": accuracy_score(vaders['Label'], vaders['Predicted_Label']),\n",
    "                                         \"Precision score\": precision_score(vaders['Label'], vaders['Predicted_Label']),\n",
    "                                         \"Recall score\": recall_score(vaders['Label'], vaders['Predicted_Label']),\n",
    "                                         \"F1 score\": f1_score(vaders['Label'], vaders['Predicted_Label'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f18515",
   "metadata": {},
   "source": [
    "### Part 4. RNN manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc3cc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary, word2index reference and convert the reviews into numerical form\n",
    "vocab_size = 10000\n",
    "\n",
    "word_counter = Counter(reviews_unrolled)\n",
    "word_counter = dict(word_counter.most_common(vocab_size))\n",
    "word2index = {k:i for i,k in enumerate(word_counter.keys(), start = 3)}\n",
    "\n",
    "# Convert reveies to integers\n",
    "reviews_int = []\n",
    "for review in reviews_tokenized:\n",
    "  cur_review = [1]\n",
    "  for word in review:\n",
    "    if word in word2index.keys():\n",
    "      cur_review.append(word2index[word])\n",
    "    else:\n",
    "      cur_review.append(2)\n",
    "  reviews_int.append(cur_review)\n",
    "  \n",
    "# Pad sequences\n",
    "padded_reviews = pad_sequences(reviews_int, maxlen = 500, padding = 'pre', truncating = 'pre')\n",
    "\n",
    "# Train test split on padded sequences\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "X_train = np.array(X_train).reshape(20000, 500)\n",
    "X_test = np.array(X_test).reshape(5000, 500)\n",
    "\n",
    "y_train = np.array(y_train).reshape(20000, 1)\n",
    "y_test = np.array(y_test).reshape(5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfc088e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 500)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 200)          333000    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 500, 200)         800       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 200)          0         \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 500, 128)          126720    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 500, 128)         512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500, 128)          0         \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 128)               99072     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               66048     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,907,177\n",
      "Trainable params: 1,906,265\n",
      "Non-trainable params: 912\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "dropout_rate = 0.5\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(filepath='best_RNN_manual_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=3, \n",
    "                           verbose=1, \n",
    "                           restore_best_weights=True)\n",
    "\n",
    "inputs = Input(shape = (X_train.shape[1:]))\n",
    "mask = tf.keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "x = Embedding(input_dim = vocab_size, output_dim = 128, input_length = 200)(inputs)\n",
    "x = Conv1D(filters = 200, kernel_size = 13, strides = 1, padding = 'same', activation = 'relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = GRU(128, return_sequences = True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = GRU(128, return_sequences = False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "x = Dense(512, activation = 'relu')(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485b8e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 23s 121ms/step - loss: 0.7199 - accuracy: 0.5634 - val_loss: 0.5821 - val_accuracy: 0.6596\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.3157 - accuracy: 0.8689 - val_loss: 0.7373 - val_accuracy: 0.6604\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 18s 118ms/step - loss: 0.1917 - accuracy: 0.9283 - val_loss: 0.3562 - val_accuracy: 0.8594\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.1372 - accuracy: 0.9495 - val_loss: 0.3817 - val_accuracy: 0.8596\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.0875 - accuracy: 0.9693 - val_loss: 0.4929 - val_accuracy: 0.8702\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9773Restoring model weights from the end of the best epoch: 3.\n",
      "157/157 [==============================] - 19s 119ms/step - loss: 0.0604 - accuracy: 0.9773 - val_loss: 0.6412 - val_accuracy: 0.8376\n",
      "Epoch 6: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "Manual_RNN_start_time = time.time()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 10, batch_size = 128, validation_data = (X_test, y_test), callbacks = [checkpoint, early_stop])\n",
    "\n",
    "Manual_RNN_end_time = time.time()\n",
    "Manual_RNN_execution_time = Manual_RNN_end_time - Manual_RNN_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07769487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 18ms/step\n",
      "Manual RNN Training time: 116.22 seconds\n",
      "Manual RNN Accuracy score:  0.8594\n",
      "Manual RNN Precision score:  0.8082191780821918\n",
      "Manual RNN Recall score:  0.936126932465419\n",
      "Manual RNN F1 score:  0.8674835061262959\n"
     ]
    }
   ],
   "source": [
    "# Model predictions\n",
    "Manual_RNN_predictions = model.predict(X_test)\n",
    "\n",
    "Manual_RNN_predictions_results = []\n",
    "for i in Manual_RNN_predictions:\n",
    "  if i[0] >= 0.5:\n",
    "    Manual_RNN_predictions_results.append(1)\n",
    "  else:\n",
    "    Manual_RNN_predictions_results.append(0)\n",
    "\n",
    "# Model evaluation\n",
    "print('Manual RNN Training time: {} seconds'. format(round(Manual_RNN_execution_time, 2)))\n",
    "print('Manual RNN Accuracy score: ', format(accuracy_score(y_test, Manual_RNN_predictions_results)))\n",
    "print('Manual RNN Precision score: ', format(precision_score(y_test, Manual_RNN_predictions_results)))\n",
    "print('Manual RNN Recall score: ', format(recall_score(y_test, Manual_RNN_predictions_results)))\n",
    "print('Manual RNN F1 score: ', format(f1_score(y_test, Manual_RNN_predictions_results)))\n",
    "\n",
    "# Model evaluation dictionary\n",
    "Manual_RNN_results = {'Name': \"Manual RNN\", \n",
    "                 \"Training Time\": round(Manual_RNN_execution_time, 2),\n",
    "                 \"Accuracy score\": accuracy_score(y_test, Manual_RNN_predictions_results),\n",
    "                 \"Precision score\": precision_score(y_test, Manual_RNN_predictions_results),\n",
    "                 \"Recall score\": recall_score(y_test, Manual_RNN_predictions_results),\n",
    "                 \"F1 score\": f1_score(y_test, Manual_RNN_predictions_results)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb5a78d",
   "metadata": {},
   "source": [
    "### Part 5. Universal Sentence encoder transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fc273d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train).reshape(20000, 1)\n",
    "y_test = np.array(y_test).reshape(5000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb0bc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 512)               256797824 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 258,373,761\n",
      "Trainable params: 1,575,937\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Download the Universal Sentence encoder layer for transfer learning\n",
    "sentence_encoder_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "                                         input_shape = [],\n",
    "                                         dtype=tf.string,\n",
    "                                         trainable = False)\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint = ModelCheckpoint(filepath='best_sentence_encoder_model.h5', \n",
    "                             monitor='val_loss', \n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           patience=3, \n",
    "                           verbose=1, \n",
    "                           restore_best_weights=True)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "  sentence_encoder_layer,\n",
    "  Dense(1024, activation = 'relu'),\n",
    "  Dense(1024, activation = 'relu'),\n",
    "  Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d6ea995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 20s 115ms/step - loss: 0.3657 - accuracy: 0.8402 - val_loss: 0.3340 - val_accuracy: 0.8614\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 17s 108ms/step - loss: 0.3158 - accuracy: 0.8647 - val_loss: 0.3223 - val_accuracy: 0.8568\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 18s 117ms/step - loss: 0.2854 - accuracy: 0.8826 - val_loss: 0.3645 - val_accuracy: 0.8442\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 17s 110ms/step - loss: 0.2507 - accuracy: 0.8988 - val_loss: 0.3213 - val_accuracy: 0.8668\n",
      "Epoch 5/10\n",
      " 71/157 [============>.................] - ETA: 8s - loss: 0.1940 - accuracy: 0.9251"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs = 10, batch_size = 128, validation_data = (X_test, y_test), callbacks = [checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"add man\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
