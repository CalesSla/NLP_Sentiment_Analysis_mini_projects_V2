{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a29412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import and data read\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "with open('reviews.txt', 'r') as f:\n",
    "  reviews = f.read()\n",
    "with open('labels.txt', 'r') as f:\n",
    "  labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534c7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "\n",
    "reviews = \"\".join([char for char in reviews if char not in string.punctuation])\n",
    "\n",
    "reviews = reviews.split('\\n')\n",
    "labels = labels.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e150d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, Lemmatization, Stemming. Label numerical encoding\n",
    "\n",
    "import itertools\n",
    "\n",
    "reviews_tokenized = []\n",
    "for review in reviews:\n",
    "  splitted_review = nltk.word_tokenize(review)\n",
    "  splitted_review = [WordNetLemmatizer().lemmatize(w) for w in splitted_review]\n",
    "  splitted_review = [PorterStemmer().stem(w).strip() for w in splitted_review]\n",
    "  reviews_tokenized.append(splitted_review)\n",
    "  \n",
    "reviews_unrolled = list(itertools.chain(*reviews_tokenized))\n",
    "  \n",
    "labels = [1 if label == \"positive\" else 0 for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7ad81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty reviews and the corresponding labels\n",
    "\n",
    "empty_idx = []\n",
    "for i, review in enumerate(reviews_tokenized):\n",
    "  if len(review) == 0:\n",
    "    empty_idx.append(i)\n",
    "    \n",
    "for i in empty_idx:\n",
    "  reviews_tokenized.pop(i)\n",
    "  reviews.pop(i)\n",
    "  labels.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75f80ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary, word2index reference and convert the reviews into numerical form\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "word_counter = Counter(reviews_unrolled)\n",
    "# word_counter = dict(word_counter.most_common(vocab_size))\n",
    "word2index = {k:i for i,k in enumerate(word_counter.keys(), start = 3)}\n",
    "\n",
    "reviews_int = []\n",
    "for review in reviews_tokenized:\n",
    "  cur_review = [1]\n",
    "  for word in review:\n",
    "    if word in word2index.keys():\n",
    "      cur_review.append(word2index[word])\n",
    "    else:\n",
    "      cur_review.append(2)\n",
    "  reviews_int.append(cur_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f542a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_reviews = pad_sequences(reviews_int, maxlen = 500, padding = 'pre', truncating = 'pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7812db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train).reshape(20000, 1)\n",
    "y_test = np.array(y_test).reshape(5000, 1)\n",
    "\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "\n",
    "X_train = preprocessor(X_train)['input_word_ids']\n",
    "X_test = preprocessor(X_test)['input_word_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f3f8b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Lambda, GlobalMaxPool1D, BatchNormalization, Dense, RNN, GRU, LSTM, TimeDistributed, Bidirectional, Activation, Embedding, Input, Conv1D, Dropout\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "sample_sentence = 'This is a bad movie'\n",
    "\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "# embed_samples = embed([sample_sentence, 'This is my second sentence'])\n",
    "\n",
    "\n",
    "# sentence_encoder_layer = hub.KerasLayer('https://tfhub.dev/google/universal-sentence-encoder/4',\n",
    "#                                          input_shape = [],\n",
    "#                                          dtype=tf.string,\n",
    "#                                          trainable = False)\n",
    "\n",
    "# sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\", \n",
    "#                                         output_shape=[256],\n",
    "#                                         input_shape=[], \n",
    "#                                         dtype=tf.string, trainable = False)\n",
    "\n",
    "\n",
    "# inputs = Input(shape = ())\n",
    "# x = preprocessor(inputs)\n",
    "# x = Dense(128, activation = 'relu')(x['['input_word_ids']'])\n",
    "# outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "# model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "  Dense(128, activation = 'relu'),\n",
    "  Dense(128, activation = 'relu'),\n",
    "  Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# inputs = Input(shape = (X_train.shape[1:]))\n",
    "# mask = tf.keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "# x = Embedding(input_dim = vocab_size, output_dim = 128, input_length = 200)(inputs)\n",
    "# x = Conv1D(filters = 200, kernel_size = 13, strides = 1, padding = 'same', activation = 'relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "# x = GRU(128, return_sequences = True)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "# x = GRU(128, return_sequences = False)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "# x = Dense(512, activation = 'relu')(x)\n",
    "# x = Dropout(dropout_rate)(x)\n",
    "# outputs = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "# model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4869803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"keras_layer_10\" \"                 f\"(type KerasLayer).\n    \n    in user code:\n    \n        File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n            result = smart_cond.smart_cond(training,\n    \n        ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n          Positional arguments (3 total):\n            * <tf.Tensor 'inputs:0' shape=(None, 128) dtype=int32>\n            * False\n            * None\n          Keyword arguments: {}\n        \n         Expected these arguments to match one of the following 4 option(s):\n        \n        Option 1:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 2:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 3:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 4:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * False\n            * None\n          Keyword arguments: {}\n    \n    \n    Call arguments received by layer \"keras_layer_10\" \"                 f\"(type KerasLayer):\n      • inputs=tf.Tensor(shape=(None, 128), dtype=int32)\n      • training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Train the model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filerqh4741r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     72\u001b[0m     result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     73\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 74\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_6\u001b[39m():\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (result,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmart_cond\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msmart_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograph_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograph_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_3.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     71\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mtrainable, if_body_2, else_body_2, get_state_2, set_state_2, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(smart_cond)\u001b[38;5;241m.\u001b[39msmart_cond, (ag__\u001b[38;5;241m.\u001b[39mld(training), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope)), ag__\u001b[38;5;241m.\u001b[39mautograph_artifact(\u001b[38;5;28;01mlambda\u001b[39;00m : ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(f), (), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), fscope))), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py\", line 74, in tf__call\n        ag__.if_stmt(ag__.not_(ag__.ld(self)._has_training_argument), if_body_3, else_body_3, get_state_3, set_state_3, ('result', 'training'), 1)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py\", line 72, in else_body_3\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\__autograph_generated_filetpbww9j2.py\", line 72, in <lambda>\n        result = ag__.converted_call(ag__.ld(smart_cond).smart_cond, (ag__.ld(training), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=True), fscope)), ag__.autograph_artifact(lambda : ag__.converted_call(ag__.ld(f), (), dict(training=False), fscope))), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"keras_layer_10\" \"                 f\"(type KerasLayer).\n    \n    in user code:\n    \n        File \"C:\\Users\\User\\anaconda3\\envs\\newtensorflow\\lib\\site-packages\\tensorflow_hub\\keras_layer.py\", line 237, in call  *\n            result = smart_cond.smart_cond(training,\n    \n        ValueError: Could not find matching concrete function to call loaded from the SavedModel. Got:\n          Positional arguments (3 total):\n            * <tf.Tensor 'inputs:0' shape=(None, 128) dtype=int32>\n            * False\n            * None\n          Keyword arguments: {}\n        \n         Expected these arguments to match one of the following 4 option(s):\n        \n        Option 1:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 2:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n            * False\n            * None\n          Keyword arguments: {}\n        \n        Option 3:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * True\n            * None\n          Keyword arguments: {}\n        \n        Option 4:\n          Positional arguments (3 total):\n            * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n            * False\n            * None\n          Keyword arguments: {}\n    \n    \n    Call arguments received by layer \"keras_layer_10\" \"                 f\"(type KerasLayer):\n      • inputs=tf.Tensor(shape=(None, 128), dtype=int32)\n      • training=True\n"
     ]
    }
   ],
   "source": [
    "# # Train the model\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs = 100, batch_size = 128, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6e330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1a5f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n",
    "# embeddings = embed([\"cat is on the mat\", \"dog is in the fog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62fff8c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# preprocessor = hub.KerasLayer(\n",
    "#     \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abf3a17f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fde87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3a80385",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61105f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = preprocessor(X_train[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920a8d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 128), dtype=int32, numpy=\n",
       "array([[   101,  14985,  16460,  24990,  32291,  14997,  14985,  19750,\n",
       "         16090,    170, 175173,  46983,  14999,  75298,  14985,  40790,\n",
       "         31782,  15308,  14997,    170, 191046,  24674,  15206,  73584,\n",
       "         18688,  35545,    170,  74357,  29161,  15014,  78152, 211814,\n",
       "        310398,  14986,  18566,  38764,  16895, 140901,  15444,  86831,\n",
       "         15127,  19155,  78152, 211814,  14999, 124087, 282517,  22897,\n",
       "        217557,  14983,  15036,  34264,  15206,  14985,  79526,  14997,\n",
       "         14985,  19750,  57247,  15141,  30510,  15053,  19559,  16596,\n",
       "         15636,  15384,  15061,  17072,  15061,  15932,  22285,  14981,\n",
       "        332259, 360845,  15277,  46186,  15272,  19750,  15206,  14985,\n",
       "         50742, 394578,  14985,  16191,  19132,  15021,  44163,  14986,\n",
       "         16512,  33865,  23099,  15205,  21117,  14999,  15808,    189,\n",
       "         21466,  15272,  19750,  21466, 332259, 360845,  15277,  14999,\n",
       "         37735,  15215,  20477,  16010,  16010,  59735, 428019,  15096,\n",
       "           170,  44751,  30491,  17669,  22285,  14999,  72236,  14986,\n",
       "         14985,  16191,  15021,  21397,  15222, 101700,  14985,    102],\n",
       "       [   101,  15272,  24814,  22793,  18383,  75971,  14999,  15127,\n",
       "         14985,  15669,  14985,  27381, 215312,  14985,  19543,  63508,\n",
       "         15015,  14985,  16706,    178, 103378,  28960,  14986,  16950,\n",
       "         14985,  73702,  14986,  14985,  18308,  15207, 334176,  15514,\n",
       "         15096,  17003,  14999, 368778,  14985,  80162,  14986,  31565,\n",
       "         15131,  15096,  43869,  15272,  16191,    170,  19621,  18344,\n",
       "         14997,  17072,  34495,  70890, 189093,  14999,  16282, 141982,\n",
       "         85720,  16351,  15143,  28476, 130128,  15933,    178,  15206,\n",
       "         14999,  16090,  15384,  25543, 109591,  14983, 204081,  15276,\n",
       "        141982,    170,  21292,  21079,  15933,  30740,  65565,  14985,\n",
       "         16388,  14986,  21466,  15272,  16191,    178,  16351, 113955,\n",
       "         34172,  15100,  39991,  14985,  15669,  15498, 349753,  15061,\n",
       "         15001,  24071,    178,  16351,  19576,  29078,  14986,  83938,\n",
       "         16051,    170, 101335,  21190,  16143,  15205, 138531,  15131,\n",
       "         15021, 257024,  16351,  15143,    170,  17072,  17922,    178,\n",
       "         33608,  15121,  15272,  16191,  15206,  15384,  14985,    102],\n",
       "       [   101,  14985,  79752,  16543,  15535,  16081, 115730, 196812,\n",
       "         14978,    188, 157799,  16543,  15140,  17115, 278887,  54850,\n",
       "         33333,  21253,  16216, 210685,  23710,  15131,  14985, 113541,\n",
       "         15394,  16068,  50927,  16290, 176092,  15207,  15179,  78234,\n",
       "         14986,    170,  19042,  15222,  19750,  15910,  46043,  15131,\n",
       "         15121,  18932,  17003, 158925,  15800, 136790,  29084,  72905,\n",
       "         16651,  15002,  53064,  15595,  14997,  14985,  97441,  15096,\n",
       "           188,  23375,  15324, 327585, 129253,  15106,  14985,  21397,\n",
       "         15002,  14997,  15001,  83754, 273068,  44424, 123680, 206243,\n",
       "        129253,  15106,  52898,  15569,  26117,  16068,  15002,  43038,\n",
       "         15014,    170,  17048,  37723,  32678,  15127,  15932,  55000,\n",
       "         15250,  65307,  15932,  52725,  15002,  14986, 200233,  14986,\n",
       "         14985,  60309,  79752,  14978,  14999,  20030,  15932,  17712,\n",
       "         21564, 150243,  99931,  41290,  15140,  40477,  15002,  15932,\n",
       "         25455,  15320, 112833, 363782,  14986,  14985, 114117,  14999,\n",
       "         25535,  14985,  98766,  14997,  23613,  31782,  15320,    102]])>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings['input_word_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15be96c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed739248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d511a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3432cd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5124098a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac7090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
